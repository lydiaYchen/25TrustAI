
#  Course descriptionTopics, papers, and supervisors

## Topic 1: GenAI 
- Supervisor: Basile
- Paper 1.1 (title)
  - Link
  - How to reproduce
  - How to extend
- Paper 1.2
  - Link
  - How to reproduce
  - How to extend


## Topic 2: RL (Matching and Multi-Armed Bandits)  

- **Supervisor:** Andreas Athanasopoulos  

- **General description:**  
  This topic aims to explore learning algorithms in the context of matching markets. Specifically, it considers a multi-agent learning scenario where multiple agents participate in a market and compete for their matches. Technically, students will study matching problems, which are central to economics (Nobel Prize awarded to Gale and Shapley), and their intersection with learning algorithms from Multi-Armed Bandits (MAB), a central framework in reinforcement learning.  

- **Paper 2.1**
  - **Link:** [https://arxiv.org/abs/1906.05363](https://arxiv.org/abs/1906.05363)  
  - **How to reproduce:**  
    To reproduce the paper, students should first implement classic Multi-Armed Bandit algorithms, specifically Explore-Then-Commit (ETC) and Upper Confidence Bound (UCB). Then, they should adapt these base algorithms to matching markets and replicate the experiments presented in the paper.  
  - **How to extend:**  
    An interesting extension is to perform experiments that empirically quantify the algorithm’s performance in the case of non-truthful agents (Section 3.3 of the paper). While the paper studies this theoretically, many questions remain regarding the extent to which an agent can exploit rewards. Initial experiments would be valuable for students and could potentially lead to a thesis. Additionally, students can research related literature to complement their work.  

- **Paper 2.2** (Challenging) 
  - **Link:** [https://arxiv.org/abs/2506.03802](https://arxiv.org/abs/2506.03802)  
  - **How to reproduce:**  
    To reproduce the paper, students should first implement a Upper Confidence Bound (UCB) algorithm for zero-sum games. Then, they can extend the UCB algorithm according to the paper and reproduce the experiments.  
 - **How to extend:**  
    1. Empirically study alternative regret measures, such as player-optimal and player-pessimal regret, independent of the matching instability studied in the paper.
    2. Conduct experiments in decentralized models. This requires students to study additional papers on decentralized matching models and perform initial experiments. Specifically, students can investigate different interaction protocols with additional feedback and analyze the system’s regret.
   3. For highly motivated students, several directions for future research can be explored.



## Topic 3: GenAI
- Supervisor: Gert
- Paper 3.1
  - Link
  - How to reproduce
  - How to exte
- Paper 3.2
  - Link
  - How to reproduce
  - How to exte

## Topic 4: RL
- Supervisor: Hortence
- Paper 4.1
  - Link
  - How to reproduce
  - How to exte
- Paper 4.2
  - Link
  - How to reproduce
  - How to exte


## Topic 5: GenAI
- Supervisor: Abel
- Paper 1.1
  - Link
  - How to reproduce
  - How to exte
- Paper 1.2
  - Link
  - How to reproduce
  - How to exte


## Topic 6 : RL
- Supervisor: Elif
- Paper 2.1
  - Link
  - How to reproduce
  - How to exte
- Paper 2.2
  - Link
  - How to reproduce
  - How to exte



- Supervisor
- Paper 8.1
- Paper 8.2
- 
